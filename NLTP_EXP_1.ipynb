{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer"
      ],
      "metadata": {
        "id": "P8Qz68XWnrLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "id": "lub_llqOntro",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf3263d5-a090-4c41-d071-f27b63744328"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "# Load dataset\n",
        "df = pd.read_csv('combined_emotion.csv', encoding='latin1', sep=',') # or ';' or '|' or other delimiter\n"
      ],
      "metadata": {
        "id": "quxZfM8Lnxv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "79nB_CRqn1Ch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    filtered_tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
        "    stemmed = [stemmer.stem(word) for word in filtered_tokens]\n",
        "    lemmatized = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
        "    return filtered_tokens, stemmed, lemmatized"
      ],
      "metadata": {
        "id": "tlr3B5n4n3TS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df[['filtered_tokens', 'stemmed', 'lemmatized']] = df['sentence'].apply(\n",
        "    lambda x: pd.Series(preprocess_text(x))\n",
        ")"
      ],
      "metadata": {
        "id": "U7l1XFt4n54Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for index, row in df.iterrows():\n",
        "    print(f\"Original Text: {row['sentence']}\") # Changed 'text' to 'Country'\n",
        "    print(f\"Filtered Tokens: {row['filtered_tokens']}\")\n",
        "    print(f\"Stemmed Tokens: {row['stemmed']}\")\n",
        "    print(f\"Lemmatized Tokens: {row['lemmatized']}\")\n",
        "    print(\"-\" * 100)"
      ],
      "metadata": {
        "id": "6wzlAL-an9nZ",
        "outputId": "e2384b3e-b3a6-41cf-dcb8-4d6a04ec78c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: France\n",
            "Filtered Tokens: ['france']\n",
            "Stemmed Tokens: ['franc']\n",
            "Lemmatized Tokens: ['france']\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Original Text: Spain\n",
            "Filtered Tokens: ['spain']\n",
            "Stemmed Tokens: ['spain']\n",
            "Lemmatized Tokens: ['spain']\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Original Text: Germany\n",
            "Filtered Tokens: ['germany']\n",
            "Stemmed Tokens: ['germani']\n",
            "Lemmatized Tokens: ['germany']\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Original Text: Spain\n",
            "Filtered Tokens: ['spain']\n",
            "Stemmed Tokens: ['spain']\n",
            "Lemmatized Tokens: ['spain']\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Original Text: Germany\n",
            "Filtered Tokens: ['germany']\n",
            "Stemmed Tokens: ['germani']\n",
            "Lemmatized Tokens: ['germany']\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Original Text: France\n",
            "Filtered Tokens: ['france']\n",
            "Stemmed Tokens: ['franc']\n",
            "Lemmatized Tokens: ['france']\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Original Text: Spain\n",
            "Filtered Tokens: ['spain']\n",
            "Stemmed Tokens: ['spain']\n",
            "Lemmatized Tokens: ['spain']\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Original Text: France\n",
            "Filtered Tokens: ['france']\n",
            "Stemmed Tokens: ['franc']\n",
            "Lemmatized Tokens: ['france']\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Original Text: Germany\n",
            "Filtered Tokens: ['germany']\n",
            "Stemmed Tokens: ['germani']\n",
            "Lemmatized Tokens: ['germany']\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Original Text: France\n",
            "Filtered Tokens: ['france']\n",
            "Stemmed Tokens: ['franc']\n",
            "Lemmatized Tokens: ['france']\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the processed dataset to a new CSV\n",
        "df.to_csv('optimized_processed_data.csv', index=False)"
      ],
      "metadata": {
        "id": "Qs-hKM1ZoAHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Processed dataset saved as 'optimized_processed_data.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2Jd2b31oCnq",
        "outputId": "bad70d2e-6854-4693-a695-e46f6cef32c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed dataset saved as 'optimized_processed_data.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def filter_tokens(sentence):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = word_tokenize(sentence)\n",
        "    return [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "\n",
        "def stem_tokens(tokens):\n",
        "    return [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "def lemmatize_tokens(tokens):\n",
        "    return [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "user_input = input(\"Enter a sentence (or type 'exit' to finish): \")\n",
        "\n",
        "data = []\n",
        "\n",
        "while user_input.lower() != 'exit':\n",
        "\n",
        "    filtered = filter_tokens(user_input)\n",
        "    stemmed = stem_tokens(filtered)\n",
        "    lemmatized = lemmatize_tokens(filtered)\n",
        "\n",
        "    data.append({\n",
        "        'sentence': user_input,\n",
        "        'filtered_tokens': filtered,\n",
        "        'stemmed': stemmed,\n",
        "        'lemmatized': lemmatized\n",
        "    })\n",
        "\n",
        "    user_input = input(\"Enter another sentence (or type 'exit' to finish): \")\n",
        "\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    print(f\"Original Text: {row['sentence']}\")\n",
        "    print(f\"Filtered Tokens: {row['filtered_tokens']}\")\n",
        "    print(f\"Stemmed Tokens: {row['stemmed']}\")\n",
        "    print(f\"Lemmatized Tokens: {row['lemmatized']}\")\n",
        "    print(\"-\" * 100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-X5zvHUpqO-o",
        "outputId": "c9347f71-231b-43d3-e7ed-99dd55202f9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a sentence (or type 'exit' to finish): i dont know i feel so lost\n",
            "Enter another sentence (or type 'exit' to finish): i can still lose the weight without feeling deprived\n",
            "Enter another sentence (or type 'exit' to finish): exit\n",
            "Original Text: i dont know i feel so lost\n",
            "Filtered Tokens: ['dont', 'know', 'feel', 'lost']\n",
            "Stemmed Tokens: ['dont', 'know', 'feel', 'lost']\n",
            "Lemmatized Tokens: ['dont', 'know', 'feel', 'lost']\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Original Text: i can still lose the weight without feeling deprived\n",
            "Filtered Tokens: ['still', 'lose', 'weight', 'without', 'feeling', 'deprived']\n",
            "Stemmed Tokens: ['still', 'lose', 'weight', 'without', 'feel', 'depriv']\n",
            "Lemmatized Tokens: ['still', 'lose', 'weight', 'without', 'feeling', 'deprived']\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jnqqxhnFv-NN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Tr4IceYcv3ae"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}